/**
 * DO NOT EDIT THIS FILE - it is machine generated
 */
package wumo.sim.algorithm.tensorflow.ops.gen

import org.bytedeco.javacpp.tensorflow.NameAttrList
import wumo.sim.algorithm.tensorflow.*
import wumo.sim.util.Dimension

fun TF.anonymousIterator(output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "AnonymousIterator") = run {
  buildOpTensor("AnonymousIterator", name) {
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.batchDataset(input_dataset: Tensor, batch_size: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "BatchDataset") = run {
  buildOpTensor("BatchDataset", name) {
    addInput(input_dataset, false)
    addInput(batch_size, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.bytesProducedStatsDataset(input_dataset: Tensor, tag: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "BytesProducedStatsDataset") = run {
  buildOpTensor("BytesProducedStatsDataset", name) {
    addInput(input_dataset, false)
    addInput(tag, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.cacheDataset(input_dataset: Tensor, filename: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "CacheDataset") = run {
  buildOpTensor("CacheDataset", name) {
    addInput(input_dataset, false)
    addInput(filename, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.concatenateDataset(input_dataset: Tensor, another_dataset: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "ConcatenateDataset") = run {
  buildOpTensor("ConcatenateDataset", name) {
    addInput(input_dataset, false)
    addInput(another_dataset, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.datasetToSingleElement(dataset: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "DatasetToSingleElement") = run {
  buildOpTensors("DatasetToSingleElement", name) {
    addInput(dataset, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.denseToSparseBatchDataset(input_dataset: Tensor, batch_size: Tensor, row_shape: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "DenseToSparseBatchDataset") = run {
  buildOpTensor("DenseToSparseBatchDataset", name) {
    addInput(input_dataset, false)
    addInput(batch_size, false)
    addInput(row_shape, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.deserializeIterator(resource_handle: Tensor, serialized: Tensor, name: String = "DeserializeIterator") = run {
  buildOp("DeserializeIterator", name) {
    addInput(resource_handle, false)
    addInput(serialized, false)
  }
}

fun TF.enqueueInQueueDataset(queue: Tensor, components: Tensor, name: String = "EnqueueInQueueDataset") = run {
  buildOp("EnqueueInQueueDataset", name) {
    addInput(queue, false)
    addInput(components, false)
  }
}

fun TF.featureStatsDataset(input_dataset: Tensor, tag: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "FeatureStatsDataset") = run {
  buildOpTensor("FeatureStatsDataset", name) {
    addInput(input_dataset, false)
    addInput(tag, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.filterDataset(input_dataset: Tensor, other_arguments: Tensor, predicate: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "FilterDataset") = run {
  buildOpTensor("FilterDataset", name) {
    addInput(input_dataset, false)
    addInput(other_arguments, false)
    attr("predicate", predicate)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.fixedLengthRecordDataset(filenames: Tensor, header_bytes: Tensor, record_bytes: Tensor, footer_bytes: Tensor, buffer_size: Tensor, name: String = "FixedLengthRecordDataset") = run {
  buildOpTensor("FixedLengthRecordDataset", name) {
    addInput(filenames, false)
    addInput(header_bytes, false)
    addInput(record_bytes, false)
    addInput(footer_bytes, false)
    addInput(buffer_size, false)
  }
}

fun TF.flatMapDataset(input_dataset: Tensor, other_arguments: Tensor, f: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "FlatMapDataset") = run {
  buildOpTensor("FlatMapDataset", name) {
    addInput(input_dataset, false)
    addInput(other_arguments, false)
    attr("f", f)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.generatorDataset(init_func_other_args: Tensor, next_func_other_args: Tensor, finalize_func_other_args: Tensor, init_func: NameAttrList, next_func: NameAttrList, finalize_func: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "GeneratorDataset") = run {
  buildOpTensor("GeneratorDataset", name) {
    addInput(init_func_other_args, false)
    addInput(next_func_other_args, false)
    addInput(finalize_func_other_args, false)
    attr("init_func", init_func)
    attr("next_func", next_func)
    attr("finalize_func", finalize_func)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.groupByWindowDataset(input_dataset: Tensor, key_func_other_arguments: Tensor, reduce_func_other_arguments: Tensor, window_size_func_other_arguments: Tensor, key_func: NameAttrList, reduce_func: NameAttrList, window_size_func: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "GroupByWindowDataset") = run {
  buildOpTensor("GroupByWindowDataset", name) {
    addInput(input_dataset, false)
    addInput(key_func_other_arguments, false)
    addInput(reduce_func_other_arguments, false)
    addInput(window_size_func_other_arguments, false)
    attr("key_func", key_func)
    attr("reduce_func", reduce_func)
    attr("window_size_func", window_size_func)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.interleaveDataset(input_dataset: Tensor, other_arguments: Tensor, cycle_length: Tensor, block_length: Tensor, f: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "InterleaveDataset") = run {
  buildOpTensor("InterleaveDataset", name) {
    addInput(input_dataset, false)
    addInput(other_arguments, false)
    addInput(cycle_length, false)
    addInput(block_length, false)
    attr("f", f)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.iterator(shared_name: String, container: String, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "Iterator") = run {
  buildOpTensor("Iterator", name) {
    attr("shared_name", shared_name)
    attr("container", container)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.iteratorFromStringHandle(string_handle: Tensor, output_types: Array<Long> = arrayOf(), output_shapes: Array<Dimension> = arrayOf(), name: String = "IteratorFromStringHandle") = run {
  buildOpTensor("IteratorFromStringHandle", name) {
    addInput(string_handle, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.iteratorGetNext(iterator: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "IteratorGetNext") = run {
  buildOpTensors("IteratorGetNext", name) {
    addInput(iterator, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.iteratorGetNextSync(iterator: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "IteratorGetNextSync") = run {
  buildOpTensors("IteratorGetNextSync", name) {
    addInput(iterator, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.iteratorToStringHandle(resource_handle: Tensor, name: String = "IteratorToStringHandle") = run {
  buildOpTensor("IteratorToStringHandle", name) {
    addInput(resource_handle, false)
  }
}

fun TF.latencyStatsDataset(input_dataset: Tensor, tag: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "LatencyStatsDataset") = run {
  buildOpTensor("LatencyStatsDataset", name) {
    addInput(input_dataset, false)
    addInput(tag, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.makeIterator(dataset: Tensor, iterator: Tensor, name: String = "MakeIterator") = run {
  buildOp("MakeIterator", name) {
    addInput(dataset, false)
    addInput(iterator, false)
  }
}

fun TF.mapDataset(input_dataset: Tensor, other_arguments: Tensor, f: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "MapDataset") = run {
  buildOpTensor("MapDataset", name) {
    addInput(input_dataset, false)
    addInput(other_arguments, false)
    attr("f", f)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.oneShotIterator(dataset_factory: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, container: String = "", shared_name: String = "", name: String = "OneShotIterator") = run {
  buildOpTensor("OneShotIterator", name) {
    attr("dataset_factory", dataset_factory)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
    attr("container", container)
    attr("shared_name", shared_name)
  }
}

fun TF.paddedBatchDataset(input_dataset: Tensor, batch_size: Tensor, padded_shapes: Array<Tensor>, padding_values: Tensor, output_shapes: Array<Dimension>, name: String = "PaddedBatchDataset") = run {
  buildOpTensor("PaddedBatchDataset", name) {
    addInput(input_dataset, false)
    addInput(batch_size, false)
    addInput(padded_shapes, false)
    addInput(padding_values, false)
    attr("output_shapes", output_shapes)
  }
}

fun TF.parallelInterleaveDataset(input_dataset: Tensor, other_arguments: Tensor, cycle_length: Tensor, block_length: Tensor, sloppy: Tensor, buffer_output_elements: Tensor, prefetch_input_elements: Tensor, f: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "ParallelInterleaveDataset") = run {
  buildOpTensor("ParallelInterleaveDataset", name) {
    addInput(input_dataset, false)
    addInput(other_arguments, false)
    addInput(cycle_length, false)
    addInput(block_length, false)
    addInput(sloppy, false)
    addInput(buffer_output_elements, false)
    addInput(prefetch_input_elements, false)
    attr("f", f)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.parallelMapDataset(input_dataset: Tensor, other_arguments: Tensor, num_parallel_calls: Tensor, f: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "ParallelMapDataset") = run {
  buildOpTensor("ParallelMapDataset", name) {
    addInput(input_dataset, false)
    addInput(other_arguments, false)
    addInput(num_parallel_calls, false)
    attr("f", f)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.prefetchDataset(input_dataset: Tensor, buffer_size: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "PrefetchDataset") = run {
  buildOpTensor("PrefetchDataset", name) {
    addInput(input_dataset, false)
    addInput(buffer_size, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.prependFromQueueAndPaddedBatchDataset(input_dataset: Tensor, batch_size: Tensor, padded_shapes: Array<Tensor>, padding_values: Tensor, output_shapes: Array<Dimension>, name: String = "PrependFromQueueAndPaddedBatchDataset") = run {
  buildOpTensor("PrependFromQueueAndPaddedBatchDataset", name) {
    addInput(input_dataset, false)
    addInput(batch_size, false)
    addInput(padded_shapes, false)
    addInput(padding_values, false)
    attr("output_shapes", output_shapes)
  }
}

fun TF.randomDataset(seed: Tensor, seed2: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "RandomDataset") = run {
  buildOpTensor("RandomDataset", name) {
    addInput(seed, false)
    addInput(seed2, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.rangeDataset(start: Tensor, stop: Tensor, step: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "RangeDataset") = run {
  buildOpTensor("RangeDataset", name) {
    addInput(start, false)
    addInput(stop, false)
    addInput(step, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.repeatDataset(input_dataset: Tensor, count: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "RepeatDataset") = run {
  buildOpTensor("RepeatDataset", name) {
    addInput(input_dataset, false)
    addInput(count, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.scanDataset(input_dataset: Tensor, initial_state: Tensor, other_arguments: Tensor, f: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "ScanDataset") = run {
  buildOpTensor("ScanDataset", name) {
    addInput(input_dataset, false)
    addInput(initial_state, false)
    addInput(other_arguments, false)
    attr("f", f)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.serializeIterator(resource_handle: Tensor, name: String = "SerializeIterator") = run {
  buildOpTensor("SerializeIterator", name) {
    addInput(resource_handle, false)
  }
}

fun TF.setStatsAggregatorDataset(input_dataset: Tensor, stats_aggregator: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "SetStatsAggregatorDataset") = run {
  buildOpTensor("SetStatsAggregatorDataset", name) {
    addInput(input_dataset, false)
    addInput(stats_aggregator, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.shuffleAndRepeatDataset(input_dataset: Tensor, buffer_size: Tensor, seed: Tensor, seed2: Tensor, count: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "ShuffleAndRepeatDataset") = run {
  buildOpTensor("ShuffleAndRepeatDataset", name) {
    addInput(input_dataset, false)
    addInput(buffer_size, false)
    addInput(seed, false)
    addInput(seed2, false)
    addInput(count, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.shuffleDataset(input_dataset: Tensor, buffer_size: Tensor, seed: Tensor, seed2: Tensor, reshuffle_each_iteration: Boolean = true, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "ShuffleDataset") = run {
  buildOpTensor("ShuffleDataset", name) {
    addInput(input_dataset, false)
    addInput(buffer_size, false)
    addInput(seed, false)
    addInput(seed2, false)
    attr("reshuffle_each_iteration", reshuffle_each_iteration)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.skipDataset(input_dataset: Tensor, count: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "SkipDataset") = run {
  buildOpTensor("SkipDataset", name) {
    addInput(input_dataset, false)
    addInput(count, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.slideDataset(input_dataset: Tensor, window_size: Tensor, window_shift: Tensor, window_stride: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "SlideDataset") = run {
  buildOpTensor("SlideDataset", name) {
    addInput(input_dataset, false)
    addInput(window_size, false)
    addInput(window_shift, false)
    addInput(window_stride, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.sparseTensorSliceDataset(indices: Tensor, values: Tensor, dense_shape: Tensor, name: String = "SparseTensorSliceDataset") = run {
  buildOpTensor("SparseTensorSliceDataset", name) {
    addInput(indices, false)
    addInput(values, false)
    addInput(dense_shape, false)
  }
}

fun TF.sqlDataset(driver_name: Tensor, data_source_name: Tensor, query: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "SqlDataset") = run {
  buildOpTensor("SqlDataset", name) {
    addInput(driver_name, false)
    addInput(data_source_name, false)
    addInput(query, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.statsAggregatorHandle(container: String = "", shared_name: String = "", name: String = "StatsAggregatorHandle") = run {
  buildOpTensor("StatsAggregatorHandle", name) {
    attr("container", container)
    attr("shared_name", shared_name)
  }
}

fun TF.statsAggregatorSummary(iterator: Tensor, name: String = "StatsAggregatorSummary") = run {
  buildOpTensor("StatsAggregatorSummary", name) {
    addInput(iterator, false)
  }
}

fun TF.tFRecordDataset(filenames: Tensor, compression_type: Tensor, buffer_size: Tensor, name: String = "TFRecordDataset") = run {
  buildOpTensor("TFRecordDataset", name) {
    addInput(filenames, false)
    addInput(compression_type, false)
    addInput(buffer_size, false)
  }
}

fun TF.takeDataset(input_dataset: Tensor, count: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "TakeDataset") = run {
  buildOpTensor("TakeDataset", name) {
    addInput(input_dataset, false)
    addInput(count, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.tensorDataset(components: Tensor, output_shapes: Array<Dimension>, name: String = "TensorDataset") = run {
  buildOpTensor("TensorDataset", name) {
    addInput(components, false)
    attr("output_shapes", output_shapes)
  }
}

fun TF.tensorSliceDataset(components: Tensor, output_shapes: Array<Dimension>, name: String = "TensorSliceDataset") = run {
  buildOpTensor("TensorSliceDataset", name) {
    addInput(components, false)
    attr("output_shapes", output_shapes)
  }
}

fun TF.textLineDataset(filenames: Tensor, compression_type: Tensor, buffer_size: Tensor, name: String = "TextLineDataset") = run {
  buildOpTensor("TextLineDataset", name) {
    addInput(filenames, false)
    addInput(compression_type, false)
    addInput(buffer_size, false)
  }
}

fun TF.unbatchDataset(input_dataset: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "UnbatchDataset") = run {
  buildOpTensor("UnbatchDataset", name) {
    addInput(input_dataset, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.zipDataset(input_datasets: Array<Tensor>, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "ZipDataset") = run {
  buildOpTensor("ZipDataset", name) {
    addInput(input_datasets, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.batchDatasetV2(input_dataset: Tensor, batch_size: Tensor, drop_remainder: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "BatchDatasetV2") = run {
  buildOpTensor("BatchDatasetV2", name) {
    addInput(input_dataset, false)
    addInput(batch_size, false)
    addInput(drop_remainder, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.datasetToGraph(input_dataset: Tensor, name: String = "DatasetToGraph") = run {
  buildOpTensor("DatasetToGraph", name) {
    addInput(input_dataset, false)
  }
}

fun TF.datasetToTFRecord(input_dataset: Tensor, filename: Tensor, compression_type: Tensor, name: String = "DatasetToTFRecord") = run {
  buildOp("DatasetToTFRecord", name) {
    addInput(input_dataset, false)
    addInput(filename, false)
    addInput(compression_type, false)
  }
}

fun TF.groupByReducerDataset(input_dataset: Tensor, key_func_other_arguments: Tensor, init_func_other_arguments: Tensor, reduce_func_other_arguments: Tensor, finalize_func_other_arguments: Tensor, key_func: NameAttrList, init_func: NameAttrList, reduce_func: NameAttrList, finalize_func: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "GroupByReducerDataset") = run {
  buildOpTensor("GroupByReducerDataset", name) {
    addInput(input_dataset, false)
    addInput(key_func_other_arguments, false)
    addInput(init_func_other_arguments, false)
    addInput(reduce_func_other_arguments, false)
    addInput(finalize_func_other_arguments, false)
    attr("key_func", key_func)
    attr("init_func", init_func)
    attr("reduce_func", reduce_func)
    attr("finalize_func", finalize_func)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.iteratorFromStringHandleV2(string_handle: Tensor, output_types: Array<Long> = arrayOf(), output_shapes: Array<Dimension> = arrayOf(), name: String = "IteratorFromStringHandleV2") = run {
  buildOpTensor("IteratorFromStringHandleV2", name) {
    addInput(string_handle, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.iteratorV2(shared_name: String, container: String, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "IteratorV2") = run {
  buildOpTensor("IteratorV2", name) {
    attr("shared_name", shared_name)
    attr("container", container)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.mapAndBatchDataset(input_dataset: Tensor, other_arguments: Tensor, batch_size: Tensor, num_parallel_batches: Tensor, drop_remainder: Tensor, f: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "MapAndBatchDataset") = run {
  buildOpTensor("MapAndBatchDataset", name) {
    addInput(input_dataset, false)
    addInput(other_arguments, false)
    addInput(batch_size, false)
    addInput(num_parallel_batches, false)
    addInput(drop_remainder, false)
    attr("f", f)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.mapAndBatchDatasetV2(input_dataset: Tensor, other_arguments: Tensor, batch_size: Tensor, num_parallel_calls: Tensor, drop_remainder: Tensor, f: NameAttrList, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "MapAndBatchDatasetV2") = run {
  buildOpTensor("MapAndBatchDatasetV2", name) {
    addInput(input_dataset, false)
    addInput(other_arguments, false)
    addInput(batch_size, false)
    addInput(num_parallel_calls, false)
    addInput(drop_remainder, false)
    attr("f", f)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.optimizeDataset(input_dataset: Tensor, optimizations: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "OptimizeDataset") = run {
  buildOpTensor("OptimizeDataset", name) {
    addInput(input_dataset, false)
    addInput(optimizations, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}

fun TF.paddedBatchDatasetV2(input_dataset: Tensor, batch_size: Tensor, padded_shapes: Array<Tensor>, padding_values: Tensor, drop_remainder: Tensor, output_shapes: Array<Dimension>, name: String = "PaddedBatchDatasetV2") = run {
  buildOpTensor("PaddedBatchDatasetV2", name) {
    addInput(input_dataset, false)
    addInput(batch_size, false)
    addInput(padded_shapes, false)
    addInput(padding_values, false)
    addInput(drop_remainder, false)
    attr("output_shapes", output_shapes)
  }
}

fun TF.sinkDataset(input_dataset: Tensor, name: String = "SinkDataset") = run {
  buildOpTensor("SinkDataset", name) {
    addInput(input_dataset, false)
  }
}

fun TF.windowDataset(input_dataset: Tensor, window_size: Tensor, output_types: Array<Long>, output_shapes: Array<Dimension>, name: String = "WindowDataset") = run {
  buildOpTensor("WindowDataset", name) {
    addInput(input_dataset, false)
    addInput(window_size, false)
    attr("output_types", output_types)
    attr("output_shapes", output_shapes)
  }
}
